{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import DirectoryLoader, ReadTheDocsLoader, UnstructuredMarkdownLoader\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Mardown Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- From Docs-components :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 5429/5484 [00:55<00:00, 98.42it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5429"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the directory containing the DITA files\n",
    "directory = \"../Markdown_content/docs-components/\"\n",
    "\n",
    "loader = DirectoryLoader(directory, loader_cls=UnstructuredMarkdownLoader, show_progress=True)\n",
    "docs_components = loader.load()\n",
    "\n",
    "len(docs_components)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- From Docs-core :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can load all the files without for loop !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 5895/5925 [00:40<00:00, 145.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5895"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"../github-content/Markdown_content/docs-core\"\n",
    "loader = DirectoryLoader(directory, loader_cls=UnstructuredMarkdownLoader, show_progress=True)\n",
    "docs_core = loader.load()\n",
    "\n",
    "len(docs_core)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- From docs_data_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 404/406 [00:02<00:00, 172.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../Markdown_content/docs-data-catalog\", loader_cls=UnstructuredMarkdownLoader, show_progress=True)\n",
    "docs_catalog = loader.load()\n",
    "len(docs_catalog)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all three documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11751"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = docs_components + docs_core + docs_catalog\n",
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "#create the length function\n",
    "def titktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text\n",
    "        )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min : 5\n",
      "Avg : 374\n",
      "50% :214.0\n",
      "75% :407.0\n",
      "90% :840.0\n",
      "95% :1308.0\n",
      "99% :2339.5\n",
      "Max : 16513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tokens_counts = [titktoken_len(doc.page_content) for doc in documents]\n",
    "\n",
    "tokens_counts = np.array(tokens_counts)\n",
    "\n",
    "print(f\"\"\"Min : {min(tokens_counts)}\n",
    "Avg : {int(sum(tokens_counts) / len(tokens_counts))}\n",
    "50% :{np.percentile(tokens_counts, 50)}\n",
    "75% :{np.percentile(tokens_counts, 75)}\n",
    "90% :{np.percentile(tokens_counts, 90)}\n",
    "95% :{np.percentile(tokens_counts, 95)}\n",
    "99% :{np.percentile(tokens_counts, 99)}\n",
    "Max : {max(tokens_counts)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15273"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\"\"\"\n",
    "    Implementation of splitting text that looks at characters. Recursively tries to split by different characters to find one that works.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=50, length_function=titktoken_len)\n",
    "documents_splitted = text_splitter.split_documents(documents)\n",
    "\n",
    "len(documents_splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15296"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\"\"\" \n",
    "    Implementation of splitting text that looks at tokens\n",
    "\"\"\"\n",
    "\n",
    "text_splitter = TokenTextSplitter(encoding_name=\"cl100k_base\", chunk_size=600, chunk_overlap=50)\n",
    "documents_splitted2 = text_splitter.split_documents(documents)\n",
    "\n",
    "len(documents_splitted2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Vectorstore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed and store documents using the vectorstore Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store the texts\n",
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "persist_directory = 'vectordb'\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "## here we are using OpenAI \n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=documents_splitted,\n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=persist_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persiste the db to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persiste the db to disk\n",
    "vectordb.persist()\n",
    "vectordb = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the persisted database from disk, and use it as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x158ac1ff0a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persist_directory = 'vectordb'\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "vectordb = Chroma(persist_directory=persist_directory, \n",
    "                  embedding_function=embedding)\n",
    "\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chroma_embeddings = pd.read_parquet('./vectordb/chroma-embeddings.parquet')\n",
    "chroma_collections = pd.read_parquet('./vectordb/chroma-collections.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15273, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>collection_uuid</th>\n",
       "      <th>uuid</th>\n",
       "      <th>embedding</th>\n",
       "      <th>document</th>\n",
       "      <th>id</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01a65d1-9561-4727-9bab-7e061a56b453</td>\n",
       "      <td>2e4243a6-96b3-4dd1-bb10-87b673cbc05b</td>\n",
       "      <td>[-0.01480712855571336, 0.011938492184529149, -...</td>\n",
       "      <td>Creating a marketing plan {#creating-a-marketi...</td>\n",
       "      <td>8783888a-0524-11ee-b6e0-f21f2e9d130d</td>\n",
       "      <td>{\"source\": \"..\\\\github-content\\\\docs_component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01a65d1-9561-4727-9bab-7e061a56b453</td>\n",
       "      <td>112b0b56-ec69-4f00-8430-02c845b29b12</td>\n",
       "      <td>[-0.013559202306646156, 0.009591309101212146, ...</td>\n",
       "      <td>Creating a product {#creating-a-product .task}...</td>\n",
       "      <td>8783888b-0524-11ee-8808-f21f2e9d130d</td>\n",
       "      <td>{\"source\": \"..\\\\github-content\\\\docs_component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01a65d1-9561-4727-9bab-7e061a56b453</td>\n",
       "      <td>e7fc43ae-5bfd-420f-8194-612b84023557</td>\n",
       "      <td>[-0.0016467134787746098, -0.000263917843337432...</td>\n",
       "      <td>Creating a support plan {#creating-a-support-p...</td>\n",
       "      <td>8783888c-0524-11ee-9cbb-f21f2e9d130d</td>\n",
       "      <td>{\"source\": \"..\\\\github-content\\\\docs_component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01a65d1-9561-4727-9bab-7e061a56b453</td>\n",
       "      <td>c389b9f9-29e4-4f05-9e4e-ea65a0f7b2b6</td>\n",
       "      <td>[-0.008271590933255035, -0.005999722609935641,...</td>\n",
       "      <td>Creating a vendor {#creating-a-vendor .task}\\n...</td>\n",
       "      <td>8783888d-0524-11ee-b3cc-f21f2e9d130d</td>\n",
       "      <td>{\"source\": \"..\\\\github-content\\\\docs_component...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01a65d1-9561-4727-9bab-7e061a56b453</td>\n",
       "      <td>79d6465a-2038-471c-8da4-ed75a63229a0</td>\n",
       "      <td>[-0.015604484891350892, 0.016436031156722604, ...</td>\n",
       "      <td>Editing a marketing plan {#editing-a-marketing...</td>\n",
       "      <td>8783888e-0524-11ee-b655-f21f2e9d130d</td>\n",
       "      <td>{\"source\": \"..\\\\github-content\\\\docs_component...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        collection_uuid                                  uuid  \\\n",
       "0  a01a65d1-9561-4727-9bab-7e061a56b453  2e4243a6-96b3-4dd1-bb10-87b673cbc05b   \n",
       "1  a01a65d1-9561-4727-9bab-7e061a56b453  112b0b56-ec69-4f00-8430-02c845b29b12   \n",
       "2  a01a65d1-9561-4727-9bab-7e061a56b453  e7fc43ae-5bfd-420f-8194-612b84023557   \n",
       "3  a01a65d1-9561-4727-9bab-7e061a56b453  c389b9f9-29e4-4f05-9e4e-ea65a0f7b2b6   \n",
       "4  a01a65d1-9561-4727-9bab-7e061a56b453  79d6465a-2038-471c-8da4-ed75a63229a0   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [-0.01480712855571336, 0.011938492184529149, -...   \n",
       "1  [-0.013559202306646156, 0.009591309101212146, ...   \n",
       "2  [-0.0016467134787746098, -0.000263917843337432...   \n",
       "3  [-0.008271590933255035, -0.005999722609935641,...   \n",
       "4  [-0.015604484891350892, 0.016436031156722604, ...   \n",
       "\n",
       "                                            document  \\\n",
       "0  Creating a marketing plan {#creating-a-marketi...   \n",
       "1  Creating a product {#creating-a-product .task}...   \n",
       "2  Creating a support plan {#creating-a-support-p...   \n",
       "3  Creating a vendor {#creating-a-vendor .task}\\n...   \n",
       "4  Editing a marketing plan {#editing-a-marketing...   \n",
       "\n",
       "                                     id  \\\n",
       "0  8783888a-0524-11ee-b6e0-f21f2e9d130d   \n",
       "1  8783888b-0524-11ee-8808-f21f2e9d130d   \n",
       "2  8783888c-0524-11ee-9cbb-f21f2e9d130d   \n",
       "3  8783888d-0524-11ee-b3cc-f21f2e9d130d   \n",
       "4  8783888e-0524-11ee-b655-f21f2e9d130d   \n",
       "\n",
       "                                            metadata  \n",
       "0  {\"source\": \"..\\\\github-content\\\\docs_component...  \n",
       "1  {\"source\": \"..\\\\github-content\\\\docs_component...  \n",
       "2  {\"source\": \"..\\\\github-content\\\\docs_component...  \n",
       "3  {\"source\": \"..\\\\github-content\\\\docs_component...  \n",
       "4  {\"source\": \"..\\\\github-content\\\\docs_component...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a01a65d1-9561-4727-9bab-7e061a56b453'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_embeddings.collection_uuid.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the chroma Retrievier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default search_type = \"similarity\"\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: You exceeded your current quota, please check your plan and billing details..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Displaying real time statistics during remote execution {#displaying-real-time-statistics-during-remote-execution_t .task}\\n\\nFrom Talend Administration Center, you can monitor in real time the execution status and performance of your processes. This allows you to identify any bottleneck during the data processing and gives you a real-time visibility on the progress of your Jobs.\\n\\nTo display the Real time statistics window during remote execution, do the following:\\n\\nIn the Menu tree view, click Job Conductor to display the list of scheduled tasks for deploying and executing Jobs on remote servers.\\n\\nEnsure the task you want to execute and display the real time statistics related has the Statistics option enabled.\\nFor more information about the activation of the statistics from Talend Administration Center, see How to activate Real Time Statistics.\\n\\nSelect it in the list of tasks and check its Status.\\nIt can be Ready to deploy or Ready to run.\\n\\nDepending on its status, click the appropriate button on the tool bar: Deploy or Run.\\nFor further information about how to generate, deploy and run a Job on Talend Administration Center, see Sequence of task execution.\\n\\nOnce the Job begins to run, the Real time statistics window pops up.\\n\\nThe following figure is an example of the Real time statistics window.\\n\\nHere, you can see the statistical information in real time as the checkpoints Job runs.\\n\\nIf the Job executed contains child Jobs, a tree view appears above the graphical view of the Job showing you all its child Jobs, along with their execution information. Here, the checkpoint job has one child Job named end. To display the parent job execution or the execution of one of its childs, simply click the corresponding node.\\n\\nIn this example, you can click the child Job node, end, to view this child Job only.\\n\\nParent topic:Accessing real time statistics', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\administration-console\\\\displaying-real-time-statistics-during-remote-execution_t.md'}),\n",
       " Document(page_content='Monitoring the execution of your Jenkins pipeline {#running-the-jenkins-pipeline_t .task}\\n\\nYou have started the execution of your pipeline.\\n\\nYour artifact repository (Nexus or Artifactory) is started.\\n\\nFrom the Jenkins home page, select TalendSimplePipeline.\\n\\nClick Open Blue Ocean in the left panel.\\n\\nClick the new line that appears on the list to see the pipeline progress.\\n\\nThe pipeline is launched and your project is being processed according to the Maven phases that have been defined in your script. The best practice is to use the deploy phase in order to generate the source code, compile it, test it, package it and then deploy the packages.\\n\\nYou can see their results displayed:\\n\\nin Jenkins: the detail of your results can be found in the logs that you can display by clicking the Display the log in new window icon.\\nExample where you can see the successful execution of the test named test_feature903:\\n\\nin Talend Cloud Management Console: example of Job and Route artifacts with version 0.1.0 deployed in the ci-workspace workspace of the dev-ci environment:\\n\\nThe option to display Git information (author, commit ID, commit date) in Talend Cloud Management Console when publishing artifacts using Continuous Integration builds is available from version 8.0.1 onwards (available from R2022-01).\\n\\nin your artifact repository: example of Job and Route artifacts with version 0.1.0 deployed in the Nexus maven-releases repository with an org.talend.ci Group ID:\\n\\nin your Docker registry: example of Job artifacts with version 0.1 and latest pushed to a Docker image called ci_image:\\n\\nParent topic:Running the Jenkins pipeline using parameters adapted to your environment', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\development-life-cycle\\\\running-the-jenkins-pipeline_t.md'}),\n",
       " Document(page_content='How to display the Job execution history {#how-to-display-the-job-execution-history_t .task}\\n\\nIn the task list of the Job Conductor, select the task you want to monitor, here it is load_california_clients_to_mysql.\\n\\nIn the Actions column, click the  icon to open the Execution History page which is filtered on the selected task.\\n\\nFor example here, you can see that the second execution ended with an error while the other executions succeeded, and that two executions are not started yet.\\nFrom the Actions column of the Execution History page, you can either execute the task in its current status, open the Error Recovery Management page where you can recover a Job which execution failed, or show the statistic view of the corresponding execution.\\nAlternatively, to open a graphical view of this Job history, click Timeline in the Talend Administration Center menu to open the corresponding page.\\n\\nParent topic:Monitoring the execution of a Job', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\administration-console\\\\how-to-display-the-job-execution-history_t.md'}),\n",
       " Document(page_content='Sequence of task execution {#sequence-of-task-execution_c .concept}\\n\\nNote: Only users that have the Operation Manager role and rights can have a read-write access to the tasks list. Other types of users can have a read-only access or no access to the list. When opening this page as a user of the Administration Center, you will have access only to the items for which you have been granted the appropriate authorization by the Administrator.\\n\\nOnce the task triggering is launched, you can follow every stage of the task sequence on the Job Conductor page of Talend Administration Center.\\n\\nYou have full control over the sequencing, as you have the possibility to launch, pause or kill a task execution at any time even though the trigger you possibly have set has already started. For more information regarding the controls over the execution, see Working with Job execution tasks.\\n\\nThe task execution sequence is made of various phases including: Job deployment, Job execution and log or error.\\n\\nThe Status changes at every stage of the task execution.\\n\\nThe execution and error status is refreshed automatically, but you can refresh the display any time by clicking Refresh on the toolbar.\\n\\nDeploying and preparing Job execution\\n\\nRunning Jobs\\n\\nErrors and Logs\\n\\nParent topic:Executing data integration Jobs from Job Conductor', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\administration-console\\\\sequence-of-task-execution_c.md'}),\n",
       " Document(page_content='Running Jobs {#running-jobs_c .concept}\\n\\nOnce the Job is deployed and installed on the relevant execution servers, the Job can thus be executed just like you would run it within Talend Studio.\\n\\nOn the Job Conductor page, you can view the Job status changing from Ready to run to Requesting run.\\n\\nIf you want to manually launch the run phase, click on the relevant button on the top toolbar. For more information about the controls, see Working with Job execution tasks.\\n\\nOnce the execution is complete, the status switches back to Ready to run. The Job can be executed again if needed.\\n\\nIn case the task did not complete properly, check the Error Status column as well as the task log for the Job completion information.\\n\\nNote: If the statistics mode is enabled for your Job, the Real time statistics window displays in front of the Job conductor page, once you click Run to execute it. For more information, see Recovering job execution.\\n\\nParent topic:Sequence of task execution', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\administration-console\\\\running-jobs_c.md'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"where can I see the execution status of my tasks in Talend Cloud? \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Removing non-matching values {#removing-non-matching-values_t .task}\\n\\nThe email pattern used on the email column showed that some records do not respect the standard email format. You can generate a ready-to-use Job to recuperate the non-matching rows from the column.\\n\\nIn the Profiling perspective, click the Analysis Results tab at the bottom of the editor.\\n\\nIn the Pattern Matching results of the email column, right-click the chart bar or the numerical results and select Generate Job.\\nThe Integration perspective opens showing the generated Job.\\n\\nThis Job uses the Extract Transform Load process to write in two separate output files the valid/invalid email rows that match/do not match the pattern.\\n\\nSave the Job and press F6 to execute it.\\n\\nThe valid and invalid rows of the email column are written in the defined output files.\\n\\nYou can replace the output files with different Talend components and recuperate the valid/invalid email rows and write them in databases for example.\\n\\nFor more information on using the Profiling perspective to identify and remove corrupt, incomplete, or inaccurate data, see Data cleansing in the Talend Studio User Guide.\\n\\nParent topic:Cleansing data', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\studio-data-profiling\\\\removing-non-matching-values_t.md'}),\n",
       " Document(page_content='Reading email addresses from a DB table and retrieving specific data {#tmysqlinvalidrows_tlogrow_reading-email-addresses-from-a-db-table-and-retrieving-specific-d_standard_component_enterprise_this-scena_c .concept}\\n\\nThis scenario applies only to Talend Data Management Platform, Talend Big Data Platform, Talend Real Time Big Data Platform, Talend Data Services Platform, Talend MDM Platform and Talend Data Fabric.\\n\\nThis scenario is a two-component Job created in Talend Studio. In this Job, tMySQLInvalidRows reads the email addresses for people from a specific country from a MySQL database table, filters data using a WHERE clause to narrow down the validation process, checks the email values against the given Talend Studio email pattern and finally extracts filtered data including the invalid rows and displays them on the console.\\n\\nBelow is the database table used in this example, some customers are from the USA and others are from Canada. The Email column contains some invalid addresses. The tMySQLInvalidRows component filters data in the Email column to read only the emails for the customers from the USA, and then validates these email addresses against the EmailAddress pattern.\\n\\nIn this scenario, we have already stored the schemas of the input table in the Repository. For more information about storing schema metadata in the Repository tree view, see Talend Studio User Guide.', metadata={'source': '..\\\\github-content\\\\docs_components\\\\standard\\\\tmysqlinvalidrows_tlogrow_reading-email-addresses-from-a-db-table-and-retrieving-specific-d_standard_component_enterprise_this-scena_c.md'}),\n",
       " Document(page_content='Removing empty and invalid records {#removing-empty-and-invalid-records_t .task}\\n\\nThe quality bar indicates that a column contains empty or invalid records.\\n\\nIn the quality bar, data that matches the column type is shown in green, while orange shows invalid data that does not match the column type. Empty records are shown in grey.\\n\\nBecause you want to focus on customers from a specific age range and specific states, empty data in the corresponding columns would be useless for you. You are simply going to remove the rows with empty data in the age and state columns, as well as removing invalid values from the dataset.\\n\\nClick the white menu icon on the top left of the grid and select Display rows with invalid or empty values.\\n\\nYou can see that this action creates a filter on your data, and only the row with empty or invalid entries from the dataset are now displayed. All the filters that are applied on your data at any moment can be seen on top of the grid.\\n\\nIn addition, you can see that a new option is available at the bottom of the functions panel. Indeed, when any filter, or condition, is applied, you have the choice to apply functions on the full data, or the filtered data only. This is Talend Data Preparation\\'s way of working with conditions.\\n\\nNow that this first filter is active on your whole data, you are going to specifically remove empty records from the age and state columns.\\n\\nClick the grey part of the quality bar in the header of the age column.\\n\\nFrom the menu that opens, select Delete the rows with empty cells.\\nNow that the empty values have been removed, the age column only contains valid data, as you can see in the quality bar for this column.\\n\\nRepeat the same operation for the state column.\\nAs revealed by the condition currently active, other columns in your dataset contain empty values, but since the focus is on the age and location, we can leave them as is. On the other hand, you will remove the invalid data from the phone number column, that can be used for marketing purposes.\\n\\nClick the orange part of the quality bar of the phone number and select Delete the rows with invalid cell.\\nAll the data that was considered invalid has now been removed.\\n\\nClick the bin icon in the filter bar to clear the filter and display the whole dataset again.\\n\\nThe rows with empty or invalid values for the age, state and phone numbers columns have been removed. Now that the dataset is a bit cleaner, you can start focusing on the data you want to put in light.\\n\\nParent topic:Using filters to create \"if\" conditions on customer data', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\data-preparation\\\\removing-empty-and-invalid-records_t.md'}),\n",
       " Document(page_content='Cleansing data {#cleansing-data_c .concept}\\n\\nAfter profiling customer data and identifying its problems, some actions should be taken on data to cleans it. You may start by generating two Talend Jobs: one to remove duplicates from the email column and the other to remove the values that do not match the email pattern.\\n\\nThis will help you see what to resolve and then you can decide what tool to use to intervene and resolve these address issues.\\n\\nRemoving duplicate values\\nAfter analyzing the email and postal columns using simple statistics indicators, the analysis results show the number of duplicate records in the columns. You can generate a ready-to-use Job on the analysis results. This Job removes duplicate values in the selected column.\\n\\nRemoving non-matching values\\nThe email pattern used on the email column showed that some records do not respect the standard email format. You can generate a ready-to-use Job to recuperate the non-matching rows from the column.', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\studio-data-profiling\\\\cleansing-data_c.md'}),\n",
       " Document(page_content='Filtering data against patterns {#filtering-data-against-patterns_t .task}\\n\\nAfter analyzing a set of columns against a group of patterns and having the results of the rows that match or do not match \"all\" the patterns, you can filter the valid/invalid data according to the used patterns.\\n\\nAn analysis of a set of columns is open in the analysis editor in the Profiling perspective of Talend Studio.\\n\\nYou have used the Java engine to execute the analysis.\\n\\nIn the analysis editor, click the Analysis Results tab at the bottom of the editor to open the detailed result view.\\n\\nClick Data to open the corresponding view.\\nA table lists the actual analyzed data in the analyzed columns.\\n\\nClick Filter Data on top of the table.\\nA dialog box is displayed listing all the patterns used in the column set analysis.\\n\\nSelect the check box(es) of the pattern(s) according to which you want to filter data.\\n\\nSelect a display option as the following:\\n|Select|To..|\\n|------|----|\\n|All data|show all analyzed data.|\\n|matches|show only the data that matches the selected pattern.|\\n|non-matches|show the data that does not match the selected pattern(s).|\\n\\nClick Finish to close the dialog box.\\n\\nIn this example, data is filtered against the Email Address pattern, and only the data that does not match is displayed.\\n\\nAll email addresses that do not match the selected pattern appear in red. Any data row that has a missing value appear with a red background.\\n\\nThe Previous and Next buttons under the table helps you to navigate back and forth through pages.\\n\\nNumbered buttons are displayed under the table to access pages directly:\\n\\nwhen you open the Data view for the first time after running the analysis,\\n\\nif you did not select a pattern in the Filter Data dialog box, or\\n\\nif you selected All data as the display option in the Filter Data dialog box.\\n\\nParent topic:Creating an analysis of a set of columns using patterns', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\studio-general\\\\filtering-data-against-patterns_t.md'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"how to filter out rows with invalid emails in Talend Studio?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Displaying real time statistics during remote execution {#displaying-real-time-statistics-during-remote-execution_t .task}\\n\\nFrom Talend Administration Center, you can monitor in real time the execution status and performance of your processes. This allows you to identify any bottleneck during the data processing and gives you a real-time visibility on the progress of your Jobs.\\n\\nTo display the Real time statistics window during remote execution, do the following:\\n\\nIn the Menu tree view, click Job Conductor to display the list of scheduled tasks for deploying and executing Jobs on remote servers.\\n\\nEnsure the task you want to execute and display the real time statistics related has the Statistics option enabled.\\nFor more information about the activation of the statistics from Talend Administration Center, see How to activate Real Time Statistics.\\n\\nSelect it in the list of tasks and check its Status.\\nIt can be Ready to deploy or Ready to run.\\n\\nDepending on its status, click the appropriate button on the tool bar: Deploy or Run.\\nFor further information about how to generate, deploy and run a Job on Talend Administration Center, see Sequence of task execution.\\n\\nOnce the Job begins to run, the Real time statistics window pops up.\\n\\nThe following figure is an example of the Real time statistics window.\\n\\nHere, you can see the statistical information in real time as the checkpoints Job runs.\\n\\nIf the Job executed contains child Jobs, a tree view appears above the graphical view of the Job showing you all its child Jobs, along with their execution information. Here, the checkpoint job has one child Job named end. To display the parent job execution or the execution of one of its childs, simply click the corresponding node.\\n\\nIn this example, you can click the child Job node, end, to view this child Job only.\\n\\nParent topic:Accessing real time statistics', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\administration-console\\\\displaying-real-time-statistics-during-remote-execution_t.md'}),\n",
       " Document(page_content='Launching the report remotely {#tdqreportrun_tlogrow_launching-the-report-remotely_standard_component_enterprise_you-have-c_t .task}\\n\\nYou have:\\n\\ncreated and configured a database in Microsfot Azure.\\n\\nconnected the database and published the Job to Talend Cloud.\\n\\nIn Talend Cloud Management Console, go to Tasks and plans.\\nThe Job published from the Studio is available.\\n\\nTo execute the Job (task), click it and click Run now.\\n\\nThe task executed successfully.\\n\\nParent topic:Launching a profiling report from Talend Cloud Management Console', metadata={'source': '..\\\\github-content\\\\docs_components\\\\standard\\\\tdqreportrun_tlogrow_launching-the-report-remotely_standard_component_enterprise_you-have-c_t.md'}),\n",
       " Document(page_content='After the Job being executed successfully, you can log into Talend MDM Web UI with any of the three newly created users to double-confirm that the sample data has been loaded.', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\studio-general\\\\making-the-mdm-demo-project-work_t.md'}),\n",
       " Document(page_content='Initial AWS Auto-Scaling launching {#initial-aws-auto-scaling-launching_t .task}\\n\\nOnce completed the above configuration, you may observe the results as suggested below:\\n\\nObserve the new TalendRuntimeAutoScaling instance created in AWS EC2 service console.\\n\\nNote: It may take 3-5 minutes for AWS Auto Scaling to spin up a new instance. In this initial step, you can only see one instance being started – as you configured one as the minimum instance number.\\n\\nLogin into your Talend Administration Center GUI and observe the Servers page.\\n\\nNote: You can see a new Runtime instance has been registered successfully in this page with server name set to its instance public IP address (completed by the addServer Talend Administration Center metaservlet).\\n\\nObserve Talend Administration Center GUI/ESB Conductor page\\n\\nNote: You may see a new ESB task has been created and deployed automatically to this new Runtime EC2 instance (completed by the saveEsbTask and requestDeployEsbTask Talend Administration Center metaservlets).\\n\\nRequest the REST web service by using Load Balancer address\\n\\nNote: Now you can observe the response from underlying EC2 Runtime instance.', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\esb\\\\initial-aws-auto-scaling-launching_t.md'}),\n",
       " Document(page_content='Running the Azure DevOps pipeline {#running-the-azure-devops-pipeline_t .task}\\n\\nYou have the possibility to run the build manually or trigger it based on changes made to your project. By default, there is no build trigger in the script provided by Talend but you can change the trigger parameter value to master if you want to automatically trigger a build whenever a commit is made on the main branch of your project for example.\\n\\nGo to Pipelines > Builds.\\n\\nEither click Queue to queue another build with the same parameters or Edit to update the azure-pipelines.yml and run a build to take these changes into account.\\n\\nOnce your pipeline has been successfully executed, you can see the detail of each step.\\n\\nThe pipeline is launched and your project is being processed according to the Maven phases that have been defined in your script. The best practice is to use the deploy phase in order to generate the source code, compile it, test it, package it and then deploy the packages.\\n\\nYou can see their results displayed:\\n\\nin the Azure DevOps pipeline detailed view: the detail of your results can be found in the logs that you can display by clicking the Maven step.\\nExample where you can see the successful execution of the test named test_feature903:\\n\\nin Talend Cloud Management Console: example of Job and Route artifacts with version 0.1.0 deployed in the ci-workspace workspace of the dev-ci environment:\\n\\nThe option to display Git information (author, commit ID, commit date) in Talend Cloud Management Console when publishing artifacts using Continuous Integration builds is available from version 8.0.1 onwards (available from R2022-01).\\n\\nin your Nexus web application or Artifactory: example of Job and Route artifacts with version 0.1.0 deployed in the maven-releases repository with an org.talend.ci Group ID:\\n\\nin your Docker registry: example of Job and Route artifacts with version 0.1 pushed in a Docker image called ci_image of a Docker registry called ci_repo:\\n\\nParent topic:Continuous Integration and Deployment using Azure DevOps', metadata={'source': '..\\\\github-content\\\\Markdown_content\\\\docs-core\\\\en\\\\development-life-cycle\\\\running-the-azure-devops-pipeline_t.md'})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Maximum Marginal Relevance Retrieval\n",
    "\n",
    "'''Maximal Marginal Relevance (MMR) aims to select the most relevant results for a given query while maximizing the diversity of the selected results.'''\n",
    "\n",
    "\n",
    "retriever_mmr = vectordb.as_retriever(search_type=\"mmr\" ,search_kwargs={\"k\": 5})\n",
    "retriever_mmr.get_relevant_documents(\"where can I see the execution status of my tasks in Talend Cloud? \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
    "    model_name='gpt-3.5-turbo'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Example using a retriever with similarity as a search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'where can I see the execution status of my tasks in Talend Cloud?',\n",
       " 'result': 'You can see the execution status of your tasks in Talend Cloud on the Job Conductor page. From there, you can monitor the execution status and performance of your processes in real time, identify any bottlenecks during data processing, and get real-time visibility on the progress of your Jobs. If you want to see the execution history of a specific task, you can select the task in the task list and click on the \"Execution History\" icon in the Actions column.'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"where can I see the execution status of my tasks in Talend Cloud?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to filter out rows with invalid emails in Talend Studio?',\n",
       " 'result': 'You can use the Profiling perspective in Talend Studio to filter out rows with invalid emails. Here are the steps:\\n\\n1. Open the analysis results for the email column in the Profiling perspective.\\n\\n2. Identify the rows with invalid email addresses either by looking at the pattern matching results or by using the quality bar.\\n\\n3. Right-click on the chart bar or the numerical results for the invalid email addresses and select \"Generate Job\".\\n\\n4. Talend Studio will generate a job that uses the Extract Transform Load process to write the valid/invalid email rows to separate output files.\\n\\n5. Save the job and execute it by pressing F6.\\n\\n6. The valid and invalid rows of the email column will be written to the defined output files.\\n\\nYou can then replace the output files with different Talend components and retrieve the valid/invalid email rows and write them to databases, for example.'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how to filter out rows with invalid emails in Talend Studio?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Example using a retriever with Maximum Marginal Relevance Retrieval as a search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the chain to answer questions \n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever_mmr, \n",
    "                                  return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'where can I see the execution status of my tasks in Talend Cloud?',\n",
       " 'result': 'You can monitor the execution status and performance of your processes in real-time from Talend Administration Center in Talend Cloud. To access this feature, you can click on \"Job Conductor\" in the Menu tree view to display the list of scheduled tasks for deploying and executing Jobs on remote servers. Once you select the task you want to execute and display the real-time statistics related to, its status will be displayed, and you can click on the appropriate button (Deploy or Run) depending on its status. Once the Job begins to run, the Real-time statistics window will pop up where you can see statistical information in real-time as the checkpoints Job runs.'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"where can I see the execution status of my tasks in Talend Cloud?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'how to filter out rows with invalid emails in Talend Studio?',\n",
       " 'result': 'You can generate a Job to filter out rows with invalid email addresses in Talend Studio. Here are the steps:\\n\\n1. In the Profiling perspective, click the Analysis Results tab at the bottom of the editor.\\n\\n2. In the Pattern Matching results of the email column, right-click the chart bar or the numerical results and select Generate Job. The Integration perspective opens showing the generated Job.\\n\\n3. This Job uses the Extract Transform Load process to write in two separate output files the valid/invalid email rows that match/do not match the pattern.\\n\\n4. Save the Job and press F6 to execute it.\\n\\n5. The valid and invalid rows of the email column are written in the defined output files.\\n\\nYou can replace the output files with different Talend components and recuperate the valid/invalid email rows and write them in databases for example.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"how to filter out rows with invalid emails in Talend Studio?\"\n",
    "qa_chain(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
